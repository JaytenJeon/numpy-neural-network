{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "numpy-multi-layer-linear-regression.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xH2WXADuSU-w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GSHqMa7avr-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def initialize_parameters(n_x, n_h, n_y, n_layer):\n",
        "    # 편의를 위해서 seed 설정\n",
        "    np.random.seed(20180930)\n",
        "    parameters = dict()\n",
        "\n",
        "    for n in range(n_layer):\n",
        "        if n+1 == 1:\n",
        "            # W1.shape = (n_x, n_h)\n",
        "            # b1.shape = (1, n_h)\n",
        "            parameters[\"W1\"] = np.random.randn(n_x, n_h) \n",
        "            parameters[\"b1\"] = np.zeros([1, n_h])\n",
        "        elif n+1 == n_layer:\n",
        "            # Wn.shape = (n_h, n_h)\n",
        "            # bn.shape = (1, n_h)\n",
        "            parameters[\"W\" + str(n + 1)] = np.random.randn(n_h, n_y)\n",
        "            parameters[\"b\" + str(n + 1)] = np.zeros([1, n_y])\n",
        "        else:\n",
        "            # WL.shape = (n_h, n_y)\n",
        "            # bL.shape = (1, n_y)\n",
        "            parameters[\"W\"+str(n+1)] = np.random.randn(n_h, n_h) \n",
        "            parameters[\"b\"+str(n+1)] = np.zeros([1, n_h])\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    caches = dict()\n",
        "    # Z1.shape = (m, n_h) = (m, n_x) * (n_x, n_h)\n",
        "    # Zn.shape = (m, n_h) = (m, n_h) * (n_h, n_h)\n",
        "    # ZL.shape = (m, n_y) = (m, n_h) * (n_h, n_y)\n",
        "    n_layer = int(len(parameters) / 2)\n",
        "    Z = X\n",
        "    for n in range(n_layer):\n",
        "        Z = np.dot(Z, parameters[\"W\"+str(n+1)]) + parameters[\"b\"+str(n+1)]\n",
        "        caches[\"Z\"+str(n+1)] = Z\n",
        "    return Z, caches\n",
        "\n",
        "\n",
        "def compute_loss(Y_hat, Y):\n",
        "    # MSE\n",
        "    return np.sum(np.square(Y_hat - Y)) / len(Y)\n",
        "\n",
        "\n",
        "def backward_propagation(X, Y, caches, paramerters):\n",
        "    # dL/dWL = dL/dZL * dZL/dWL\n",
        "    # dL/dbL = dL/dZL * dZL/dbL\n",
        "\n",
        "    # dL/dWn = dL/dZL * dZL/dZL-1 * dZL-1/dZL-2 * ... * dZn/dWn\n",
        "    # dL/dbn = dL/dZL * dZL/dZL-1 * dZL-1/dZL-2 * ... * dZn/dbn\n",
        "\n",
        "    # by Chain rule\n",
        "    \n",
        "    grads = dict()\n",
        "    n_layer = len(caches)\n",
        "    for n in reversed(range(n_layer)):\n",
        "      \n",
        "        if n+1 == n_layer:\n",
        "            dL_dZ = 2 * (caches[\"Z\"+str(n+1)] - Y)\n",
        "        else:\n",
        "            dL_dZ = np.dot(dL_dZ, parameters[\"W\"+str(n+2)].T)\n",
        "        \n",
        "        if n == 0:\n",
        "          dZ_dW = X\n",
        "        else:\n",
        "          dZ_dW = caches[\"Z\"+str(n)]\n",
        "\n",
        "        dZ_db = 1\n",
        "        dL_dW = np.dot(dL_dZ.T, dZ_dW)\n",
        "        dL_db = np.sum(dL_dZ * dZ_db)\n",
        "\n",
        "        grads[\"dW\"+str(n+1)] = dL_dW\n",
        "        grads[\"db\"+str(n+1)] = dL_db\n",
        "    return grads\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    n_layer = int(len(parameters) / 2)\n",
        "    for n in range(n_layer):\n",
        "        parameters[\"W\"+str(n+1)] -= learning_rate * grads[\"dW\"+str(n+1)].T\n",
        "        parameters[\"b\"+str(n+1)] -= learning_rate * grads[\"db\"+str(n+1)]\n",
        "\n",
        "    return parameters\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vOax8eLGvuig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "78f0ce1d-a476-4f04-a4ef-892990a8af3e"
      },
      "cell_type": "code",
      "source": [
        "# Data. X.shape = (4, 3), Y.shape = (4, 1)\n",
        "X = np.array([[100, 90, 95], [85, 75, 75], [100, 100, 100], [50, 40, 45]])\n",
        "Y = np.array([[95], [80], [100], [50]])\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[100  90  95]\n",
            " [ 85  75  75]\n",
            " [100 100 100]\n",
            " [ 50  40  45]]\n",
            "[[ 95]\n",
            " [ 80]\n",
            " [100]\n",
            " [ 50]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xl5wgZ4muw6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "f39b783f-c34a-4609-c24f-57ed6c8d58cd"
      },
      "cell_type": "code",
      "source": [
        "# Hyperparamerters\n",
        "num_epochs = 1000\n",
        "learning_rate = 1e-7\n",
        "num_layers = 4\n",
        "\n",
        "\n",
        "# 1. Initialize Parameters\n",
        "parameters = initialize_parameters(X.shape[1], 4, Y.shape[1], num_layers)\n",
        "\n",
        "# 2. Loop N iteration (N: Num of epochs)\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward Probagation\n",
        "    Y_hat, caches = forward_propagation(X, parameters)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(Y_hat, Y)\n",
        "\n",
        "    # Backward Propagation\n",
        "    grads = backward_propagation(X, Y, caches, parameters)\n",
        "\n",
        "    # Update Parameters\n",
        "    parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    # Print Loss\n",
        "    if (epoch + 1) % 100 == 0 or epoch + 1 == 1:\n",
        "        print(epoch + 1, loss)\n",
        "\n",
        "print(Y)\n",
        "print(Y_hat)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 64810.11985222623\n",
            "100 8.057156577070987\n",
            "200 8.003410259104774\n",
            "300 7.950627130446567\n",
            "400 7.898786826536159\n",
            "500 7.847869477606102\n",
            "600 7.79785569460717\n",
            "700 7.748726555606734\n",
            "800 7.700463592635301\n",
            "900 7.653048778970677\n",
            "1000 7.606464516834798\n",
            "[[ 95]\n",
            " [ 80]\n",
            " [100]\n",
            " [ 50]]\n",
            "[[ 98.40365604]\n",
            " [ 76.16857999]\n",
            " [100.66448458]\n",
            " [ 48.07135679]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}