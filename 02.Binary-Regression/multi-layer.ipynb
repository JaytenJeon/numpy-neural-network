{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "xH2WXADuSU-w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GSHqMa7avr-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(input):\n",
        "  return 1 / (1+np.exp(-input))\n",
        "\n",
        "\n",
        "def relu(input):\n",
        "  return input * (input > 0)\n",
        "\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y, n_layer):\n",
        "    # 편의를 위해서 seed 설정\n",
        "    np.random.seed(20181001)\n",
        "    parameters = dict()\n",
        "\n",
        "    for n in range(n_layer):\n",
        "        if n+1 == 1:\n",
        "            # W1.shape = (n_x, n_h)\n",
        "            # b1.shape = (1, n_h)\n",
        "            parameters[\"W1\"] = np.random.randn(n_x, n_h) \n",
        "            parameters[\"b1\"] = np.zeros([1, n_h])\n",
        "        elif n+1 == n_layer:\n",
        "            # Wn.shape = (n_h, n_h)\n",
        "            # bn.shape = (1, n_h)\n",
        "            parameters[\"W\" + str(n + 1)] = np.random.randn(n_h, n_y)\n",
        "            parameters[\"b\" + str(n + 1)] = np.zeros([1, n_y])\n",
        "        else:\n",
        "            # WL.shape = (n_h, n_y)\n",
        "            # bL.shape = (1, n_y)\n",
        "            parameters[\"W\"+str(n+1)] = np.random.randn(n_h, n_h) \n",
        "            parameters[\"b\"+str(n+1)] = np.zeros([1, n_h])\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def forward_propagation(X, parameters, activation=\"sigmoid\"):\n",
        "    caches = dict()\n",
        "    # Z1.shape = (m, n_h) = (m, n_x) * (n_x, n_h)\n",
        "    # Zn.shape = (m, n_h) = (m, n_h) * (n_h, n_h)\n",
        "    # ZL.shape = (m, n_y) = (m, n_h) * (n_h, n_y)\n",
        "    n_layer = int(len(parameters) / 2)\n",
        "    A = X\n",
        "    for n in range(n_layer):\n",
        "        Z = np.dot(A, parameters[\"W\"+str(n+1)]) + parameters[\"b\"+str(n+1)]\n",
        "        A = sigmoid(Z)\n",
        "        caches[\"A\"+str(n+1)] = A\n",
        "    return A, caches\n",
        "\n",
        "\n",
        "def compute_loss(Y_hat, Y):\n",
        "  return -np.sum(Y*np.log(Y_hat) + (1-Y) * np.log(1-Y_hat)) / len(Y)\n",
        "\n",
        "\n",
        "def backward_propagation(X, Y, caches, paramerters):\n",
        "    # dL/dWL = dL/dZL * dZL/dWL\n",
        "    # dL/dbL = dL/dZL * dZL/dbL\n",
        "\n",
        "    # dL/dWn = dL/dZL * dZL/dZL-1 * dZL-1/dZL-2 * ... * dZn/dWn\n",
        "    # dL/dbn = dL/dZL * dZL/dZL-1 * dZL-1/dZL-2 * ... * dZn/dbn\n",
        "\n",
        "    # by Chain rule\n",
        "    \n",
        "    grads = dict()\n",
        "    n_layer = len(caches)\n",
        "    for n in reversed(range(n_layer)):\n",
        "      \n",
        "        if n+1 == n_layer:\n",
        "            dL_dZ = (caches[\"A\"+str(n+1)] - Y)\n",
        "        else:\n",
        "            dL_dZ = np.dot(dL_dZ, parameters[\"W\"+str(n+2)].T) * caches[\"A\"+str(n+1)] * (1-caches[\"A\"+str(n+1)])\n",
        "        \n",
        "        if n == 0:\n",
        "          dZ_dW = X\n",
        "        else:\n",
        "          dZ_dW = caches[\"A\"+str(n)]\n",
        "\n",
        "        dZ_db = 1\n",
        "        dL_dW = np.dot(dL_dZ.T, dZ_dW)\n",
        "        dL_db = np.sum(dL_dZ * dZ_db)\n",
        "\n",
        "        grads[\"dW\"+str(n+1)] = dL_dW\n",
        "        grads[\"db\"+str(n+1)] = dL_db\n",
        "    return grads\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    n_layer = int(len(parameters) / 2)\n",
        "    for n in range(n_layer):\n",
        "        parameters[\"W\"+str(n+1)] -= learning_rate * grads[\"dW\"+str(n+1)].T\n",
        "        parameters[\"b\"+str(n+1)] -= learning_rate * grads[\"db\"+str(n+1)]\n",
        "\n",
        "    return parameters\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vOax8eLGvuig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "1fdf46a7-4688-4131-f152-3761b4a5fcdf"
      },
      "cell_type": "code",
      "source": [
        "# Data. X.shape = (4, 3), Y.shape = (4, 1)\n",
        "X = np.array([[1, 2], [3, 4],[2, 1],[4, 3]])\n",
        "Y = np.array([[0], [1], [0], [1] ])\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]\n",
            " [2 1]\n",
            " [4 3]]\n",
            "[[0]\n",
            " [1]\n",
            " [0]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xl5wgZ4muw6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "17d5b257-d091-4673-80c5-2eb9f1552310"
      },
      "cell_type": "code",
      "source": [
        "# Hyperparamerters\n",
        "num_epochs = 1000\n",
        "learning_rate = 1e-1\n",
        "num_layers = 4\n",
        "\n",
        "\n",
        "# 1. Initialize Parameters\n",
        "parameters = initialize_parameters(X.shape[1], 4, Y.shape[1], num_layers)\n",
        "\n",
        "# 2. Loop N iteration (N: Num of epochs)\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward Probagation\n",
        "    Y_hat, caches = forward_propagation(X, parameters)\n",
        "    # Compute loss\n",
        "    loss = compute_loss(Y_hat, Y)\n",
        "\n",
        "    # Backward Propagation\n",
        "    grads = backward_propagation(X, Y, caches, parameters)\n",
        "\n",
        "    # Update Parameters\n",
        "    parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    # Print Loss\n",
        "    if (epoch + 1) % 100 == 0 or epoch + 1 == 1:\n",
        "        print(epoch + 1, loss)\n",
        "\n",
        "print(Y)\n",
        "print(Y_hat)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 0.984586941732039\n",
            "100 0.6901410234359231\n",
            "200 0.6715910020219296\n",
            "300 0.62329238525814\n",
            "400 0.4262446375956702\n",
            "500 0.11753236201251946\n",
            "600 0.04956169950882246\n",
            "700 0.029602242105562265\n",
            "800 0.020652003416822298\n",
            "900 0.015682585441169607\n",
            "1000 0.012557580548105487\n",
            "[[0]\n",
            " [1]\n",
            " [0]\n",
            " [1]]\n",
            "[[0.01016205]\n",
            " [0.9852055 ]\n",
            " [0.01019634]\n",
            " [0.98524731]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8H4YOiBDxdz3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}